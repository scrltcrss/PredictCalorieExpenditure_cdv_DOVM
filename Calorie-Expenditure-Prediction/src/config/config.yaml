data:
  train: data/train.csv
  test: data/test.csv

training:
  epochs: 100
  batch_size: 256
  learning_rate: 0.001
  weight_decay: 0.0

  n_folds: 5
  random_state: 42

  patience: 10

  lr_factor: 0.5
  lr_patience: 3
  min_lr: 0.000001

  use_log_transform: true
  normalize_features: true

  momentum: 0.9

experiments:
  architectures: ['simple', 'deep', 'wide', 'residual', 'adaptive']

  dropout_rates: [0.0, 0.2]

  learning_rates: [0.0005, 0.001, 0.002]

  batch_sizes: [256]

  output_dir: experiments